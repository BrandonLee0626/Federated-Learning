{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tS-CRBVf5cV_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNPP9_Vu65oP",
        "outputId": "18078be9-6fae-4589-a84d-076494df7259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyTorch version: 2.3.0  Device: cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "print(\"Using PyTorch version:\", torch.__version__,' Device:', DEVICE)\n",
        "\n",
        "BATCH_SIZE = int(input('BATCH_SIZE: '))\n",
        "\n",
        "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
        "                               train = True,\n",
        "                               download = True,\n",
        "                               transform = transforms.ToTensor())\n",
        "\n",
        "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
        "                              train = False,\n",
        "                              transform = transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = BATCH_SIZE,\n",
        "                                           shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                          batch_size = BATCH_SIZE,\n",
        "                                          shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EsIlDoZr68ce"
      },
      "outputs": [],
      "source": [
        "def creat_clients(num_clients=10, initial='clients'):\n",
        "    client_names = [f'{initial}_{i+1}' for i in range(num_clients)]\n",
        "\n",
        "    size = len(train_dataset) // num_clients\n",
        "    shards = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                         batch_size = size,\n",
        "                                         shuffle = True)\n",
        "\n",
        "    assert(len(shards) == len(client_names))\n",
        "\n",
        "    return {client_names[i] : data for (i, data) in enumerate(shards)}\n",
        "\n",
        "clients = creat_clients()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_kQmbadg6_kb"
      },
      "outputs": [],
      "source": [
        "def batch_data(data_shard, BATCH_SIZE):\n",
        "    dataset = torch.utils.data.TensorDataset(data_shard[0], data_shard[1])\n",
        "    return torch.utils.data.DataLoader(dataset = dataset,\n",
        "                                       batch_size = BATCH_SIZE,\n",
        "                                       shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GghrVtZt7Dz2"
      },
      "outputs": [],
      "source": [
        "clients_batched = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data, BATCH_SIZE=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VKTQH0jJ7GTQ"
      },
      "outputs": [],
      "source": [
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "\n",
        "    global_count = sum([len(clients_trn_data[client_name]) for client_name in client_names])\n",
        "    local_count = len(clients_trn_data[client_name])\n",
        "\n",
        "    return local_count/global_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0b1wHHTH7H7y"
      },
      "outputs": [],
      "source": [
        "def scale_model_parameter(parameter, scalar):\n",
        "    return parameter*scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qmh32ubv7KM4"
      },
      "outputs": [],
      "source": [
        "def sum_scaled_weights(scaled_weights_list):\n",
        "    return sum(scaled_weights_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sPttCUhZ7L7Y"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.fc1(x)\n",
        "        x = F.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.sigmoid(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TTNxv5-97OMv"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rK8iyP-oN8uu"
      },
      "outputs": [],
      "source": [
        "def FedSGD(global_model, global_optimizer, EPOCHS=10, comm_rounds=10):\n",
        "    for comm_round in range(comm_rounds):\n",
        "        global_weight = {'fc1': global_model.fc1.weight.data, 'fc2':global_model.fc2.weight.data, 'fc3': global_model.fc3.weight.data}\n",
        "        global_bias = {'fc1': global_model.fc1.bias.data, 'fc2': global_model.fc2.bias.data, 'fc3': global_model.fc3.bias.data}\n",
        "\n",
        "        scaled_local_fc1_wieght_list = list()\n",
        "        scaled_local_fc1_bias_list = list()\n",
        "        scaled_local_fc2_wieght_list = list()\n",
        "        scaled_local_fc2_bias_list = list()\n",
        "        scaled_local_fc3_wieght_list = list()\n",
        "        scaled_local_fc3_bias_list = list()\n",
        "\n",
        "        client_names = list(clients_batched.keys())\n",
        "        random.shuffle(client_names)\n",
        "\n",
        "        for client_name in client_names:\n",
        "            local_model = Net().to(DEVICE)\n",
        "            local_model.fc1.weight.data = global_weight['fc1']\n",
        "            local_model.fc1.bias.data = global_bias['fc1']\n",
        "            local_model.fc2.weight.data = global_weight['fc2']\n",
        "            local_model.fc2.bias.data = global_bias['fc2']\n",
        "            local_model.fc3.weight.data = global_weight['fc3']\n",
        "            local_model.fc3.bias.data = global_bias['fc3']\n",
        "\n",
        "            for EPOCH in range(EPOCHS):\n",
        "              train(local_model, clients_batched[client_name], global_optimizer)\n",
        "\n",
        "            scaling_factor = weight_scalling_factor(clients_batched, client_name)\n",
        "\n",
        "            scaled_fc1_weight = scale_model_parameter(local_model.fc1.weight.data, scaling_factor)\n",
        "            scaled_local_fc1_wieght_list.append(scaled_fc1_weight)\n",
        "            scaled_fc1_bias = scale_model_parameter(local_model.fc1.bias.data, scaling_factor)\n",
        "            scaled_local_fc1_bias_list.append(scaled_fc1_bias)\n",
        "\n",
        "            scaled_fc2_weight = scale_model_parameter(local_model.fc2.weight.data, scaling_factor)\n",
        "            scaled_local_fc2_wieght_list.append(scaled_fc2_weight)\n",
        "            scaled_fc2_bias = scale_model_parameter(local_model.fc2.bias.data, scaling_factor)\n",
        "            scaled_local_fc2_bias_list.append(scaled_fc2_bias)\n",
        "\n",
        "            scaled_fc3_weight = scale_model_parameter(local_model.fc3.weight.data, scaling_factor)\n",
        "            scaled_local_fc3_wieght_list.append(scaled_fc3_weight)\n",
        "            scaled_fc3_bias = scale_model_parameter(local_model.fc3.bias.data, scaling_factor)\n",
        "            scaled_local_fc3_bias_list.append(scaled_fc3_bias)\n",
        "\n",
        "        average_fc1_weight = sum_scaled_weights(scaled_local_fc1_wieght_list)\n",
        "        global_model.fc1.weight.data = average_fc1_weight\n",
        "        average_fc1_bias = sum_scaled_weights(scaled_local_fc1_bias_list)\n",
        "        global_model.fc1.bias.data = average_fc1_bias\n",
        "\n",
        "        average_fc2_weight = sum_scaled_weights(scaled_local_fc2_wieght_list)\n",
        "        global_model.fc2.weight.data = average_fc2_weight\n",
        "        average_fc2_bias = sum_scaled_weights(scaled_local_fc2_bias_list)\n",
        "        global_model.fc2.bias.data = average_fc2_bias\n",
        "\n",
        "        average_fc3_weight = sum_scaled_weights(scaled_local_fc3_wieght_list)\n",
        "        global_model.fc3.weight.data = average_fc3_weight\n",
        "        average_fc3_bias = sum_scaled_weights(scaled_local_fc3_bias_list)\n",
        "        global_model.fc3.bias.data = average_fc3_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HONSghKd7P_q"
      },
      "outputs": [],
      "source": [
        "def FedAvg(global_model, global_optimizer, C, EPOCHS=10, comm_rounds=10):\n",
        "    for comm_round in range(comm_rounds):\n",
        "        global_weight = {'fc1': global_model.fc1.weight.data, 'fc2':global_model.fc2.weight.data, 'fc3': global_model.fc3.weight.data}\n",
        "        global_bias = {'fc1': global_model.fc1.bias.data, 'fc2': global_model.fc2.bias.data, 'fc3': global_model.fc3.bias.data}\n",
        "\n",
        "        scaled_local_fc1_wieght_list = list()\n",
        "        scaled_local_fc1_bias_list = list()\n",
        "        scaled_local_fc2_wieght_list = list()\n",
        "        scaled_local_fc2_bias_list = list()\n",
        "        scaled_local_fc3_wieght_list = list()\n",
        "        scaled_local_fc3_bias_list = list()\n",
        "\n",
        "        m = max(C*len(clients_batched), 1)\n",
        "\n",
        "        client_names = list(clients_batched.keys())\n",
        "        client_names = random.sample(client_names, m)\n",
        "\n",
        "        for client_name in client_names:\n",
        "            local_model = Net().to(DEVICE)\n",
        "            local_model.fc1.weight.data = global_weight['fc1']\n",
        "            local_model.fc1.bias.data = global_bias['fc1']\n",
        "            local_model.fc2.weight.data = global_weight['fc2']\n",
        "            local_model.fc2.bias.data = global_bias['fc2']\n",
        "            local_model.fc3.weight.data = global_weight['fc3']\n",
        "            local_model.fc3.bias.data = global_bias['fc3']\n",
        "\n",
        "            for EPOCH in EPOCHS:\n",
        "              train(local_model, clients_batched[client_name], global_optimizer)\n",
        "\n",
        "            scaling_factor = weight_scalling_factor(clients_batched, client_name)\n",
        "\n",
        "            scaled_fc1_weight = scale_model_parameter(local_model.fc1.weight.data, scaling_factor)\n",
        "            scaled_local_fc1_wieght_list.append(scaled_fc1_weight)\n",
        "            scaled_fc1_bias = scale_model_parameter(local_model.fc1.bias.data, scaling_factor)\n",
        "            scaled_local_fc1_bias_list.append(scaled_fc1_bias)\n",
        "\n",
        "            scaled_fc2_weight = scale_model_parameter(local_model.fc2.weight.data, scaling_factor)\n",
        "            scaled_local_fc2_wieght_list.append(scaled_fc2_weight)\n",
        "            scaled_fc2_bias = scale_model_parameter(local_model.fc2.bias.data, scaling_factor)\n",
        "            scaled_local_fc2_bias_list.append(scaled_fc2_bias)\n",
        "\n",
        "            scaled_fc3_weight = scale_model_parameter(local_model.fc3.weight.data, scaling_factor)\n",
        "            scaled_local_fc3_wieght_list.append(scaled_fc3_weight)\n",
        "            scaled_fc3_bias = scale_model_parameter(local_model.fc3.bias.data, scaling_factor)\n",
        "            scaled_local_fc3_bias_list.append(scaled_fc3_bias)\n",
        "\n",
        "        average_fc1_weight = sum_scaled_weights(scaled_local_fc1_wieght_list)\n",
        "        global_model.fc1.weight.data = average_fc1_weight\n",
        "        average_fc1_bias = sum_scaled_weights(scaled_local_fc1_bias_list)\n",
        "        global_model.fc1.bias.data = average_fc1_bias\n",
        "\n",
        "        average_fc2_weight = sum_scaled_weights(scaled_local_fc2_wieght_list)\n",
        "        global_model.fc2.weight.data = average_fc2_weight\n",
        "        average_fc2_bias = sum_scaled_weights(scaled_local_fc2_bias_list)\n",
        "        global_model.fc2.bias.data = average_fc2_bias\n",
        "\n",
        "        average_fc3_weight = sum_scaled_weights(scaled_local_fc3_wieght_list)\n",
        "        global_model.fc3.weight.data = average_fc3_weight\n",
        "        average_fc3_bias = sum_scaled_weights(scaled_local_fc3_bias_list)\n",
        "        global_model.fc3.bias.data = average_fc3_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LykuDFB-7SKk"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            output = model(image)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            prediction = output.max(1, keepdim=True)[1]\n",
        "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
        "\n",
        "    test_loss /= (len(test_loader.dataset)/ BATCH_SIZE)\n",
        "    test_accuracy = 100. * correct/len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KVzfsTgTG_z"
      },
      "outputs": [],
      "source": [
        "baseline_model = Net().to(DEVICE)\n",
        "baseline_optimizer = torch.optim.SGD(baseline_model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "test_accuracies = list()\n",
        "\n",
        "for comm_rounds in range(0,50, 5):\n",
        "  FedSGD(baseline_model, baseline_optimizer, comm_rounds=comm_rounds)\n",
        "  test_loss, test_accuracy = evaluate(baseline_model, test_loader)\n",
        "  test_accuracies.append(test_accuracy)\n",
        "\n",
        "plt.plot(test_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_NPY6Ra7UBT"
      },
      "outputs": [],
      "source": [
        "global_model = Net().to(DEVICE)\n",
        "global_optimizer = torch.optim.SGD(global_model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "global_test_accuracies = list()\n",
        "\n",
        "for comm_rounds in range(0,50, 5):\n",
        "  FedAvg(global_model, global_optimizer, comm_rounds=comm_rounds)\n",
        "  test_loss, test_accuracy = evaluate(global_model, test_loader)\n",
        "  global_test_accuracies.append(test_accuracy)\n",
        "\n",
        "plt.plot(global_test_accuracies)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
